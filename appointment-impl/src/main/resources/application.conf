#
#
play.application.loader = com.hiessy.appointment.impl.AppointmentLoader

appointment.cassandra.keyspace = appointment

cassandra-journal.keyspace = ${appointment.cassandra.keyspace}
cassandra-snapshot-store.keyspace = ${appointment.cassandra.keyspace}
lagom.persistence.read-side.cassandra.keyspace = ${appointment.cassandra.keyspace}


# The properties below override Lagom default configuration with the recommended values for new projects.
#
# Lagom has not yet made these settings the defaults for backward-compatibility reasons.

# Prefer 'ddata' over 'persistence' to share cluster sharding state for new projects.
# See https://doc.akka.io/docs/akka/current/cluster-sharding.html#distributed-data-vs-persistence-mode
akka.cluster.sharding.state-store-mode = ddata

# Enable the serializer provided in Akka 2.5.8+ for akka.Done and other internal
# messages to avoid the use of Java serialization.
akka.actor.serialization-bindings {
  "akka.Done"                 = akka-misc
  "akka.actor.Address"        = akka-misc
  "akka.remote.UniqueAddress" = akka-misc
}

aws {
    msk {
        brokers{
            node1 = "b-2.mosaic-engineering-san.tr9xqe.c1.kafka.us-east-1.amazonaws.com:9092,",
            node2 = "b-1.mosaic-engineering-san.tr9xqe.c1.kafka.us-east-1.amazonaws.com:9092,",
            node3 = "b-3.mosaic-engineering-san.tr9xqe.c1.kafka.us-east-1.amazonaws.com:9092",
        }
        nodes = ${aws.msk.brokers.node1} ${aws.msk.brokers.node2} ${aws.msk.brokers.node3}
    }
    s3 {
        bucketName = "mosaic-credit-reports-testing"
        bucketName = ${?S3_BUCKET_NAME}
    }
}

lagom.broker.kafka {
  # The name of the Kafka service to look up out of the service locator.
  # If this is an empty string, then a service locator lookup will not be done,
  # and the brokers configuration will be used instead.
  service-name = "msk_brokers"
  service-name = ${?KAFKA_SERVICE_NAME}

  # The URLs of the Kafka brokers. Separate each URL with a comma.
  # This will be ignored if the service-name configuration is non empty.
  brokers = "localHost:9092"

  client {
    default {
      # Exponential backoff for failures
      failure-exponential-backoff {
        # minimum (initial) duration until processor is started again
        # after failure
        min = 3s

        # the exponential back-off is capped to this duration
        max = 30s

        # additional random delay is based on this factor
        random-factor = 0.2
      }
    }

    # configuration used by the Lagom Kafka consumer
    consumer {
      failure-exponential-backoff = ${lagom.broker.kafka.client.default.failure-exponential-backoff}

      # The number of offsets that will be buffered to allow the consumer flow to
      # do its own buffering. This should be set to a number that is at least as
      # large as the maximum amount of buffering that the consumer flow will do,
      # if the consumer buffer buffers more than this, the offset buffer will
      # backpressure and cause the stream to stop.
      offset-buffer = 100

      # Number of messages batched together by the consumer before the related messages'
      # offsets are committed to Kafka.
      # By increasing the batching-size you are trading speed with the risk of having
      # to re-process a larger number of messages if a failure occurs.
      # The value provided must be strictly greater than zero.
      batching-size = 20

      # Interval of time waited by the consumer before the currently batched messages'
      # offsets are committed to Kafka.
      # This parameter is useful to ensure that messages' offsets are always committed
      # within a fixed amount of time.
      # The value provided must be strictly greater than zero.
      batching-interval = 5 seconds

      # Parallelsim for async committing to Kafka
      # The value provided must be strictly greater than zero.
      batching-parallelism = 3
    }
  }
}